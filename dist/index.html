<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>A Shallow Dive Into Deep Learning</title>
  <style>
*{box-sizing:border-box}body{font-family:Arial,Helvetica,sans-serif}h1,h2,h3,h4,h5,h6{margin:0 0 20px 0;font-family:Arial,Helvetica,sans-serif}h1{color:#3A89C9}h2{color:#F26C4F}h3{color:#757575}li{margin-bottom:.25em}pre,code{text-align:left;font-family:"Lucida Console",Monaco,monospace;color:#F26C4F;background:#F8F8F8}a,a:visited,a:hover,a:active{color:#212121}img{vertical-align:inherit}blockquote{border-left:8px solid;padding-left:.5em;color:#757575;text-align:left;margin:1em 0}blockquote>p{margin:0}.remark-code{font-size:.9em}.remark-container{background:#333}.remark-slide-scaler{box-shadow:none}.remark-notes{font-size:1.5em}.remark-slide-content{font-size:28px;padding:1em 2em;color:#212121;background-size:cover}.remark-slide-content.big-title-slide h1{font-size:100px}.remark-slide-content.big-title-slide ol{padding-left:100px}.remark-slide-content.big-title-slide li{font-size:80px}.remark-slide-number{color:#fff;right:1em;opacity:.6;font-size:0.8em;z-index:2}.no-counter .remark-slide-number{display:none}.impact{background-color:#3A89C9;vertical-align:middle;text-align:center}.impact,.impact h1,.impact h2{color:#fff}.impact h1{font-size:128px}.impact.red{background-color:#F26C4F}.impact.dark-purple{background-color:#811d5e}.impact.light-purple{background-color:#983275}.impact.yellow{background-color:#fed800}.impact.yellow h1,.impact.yellow h2{color:#811d5e}.full,.full h1,.full h2{color:#fff}.fulliframe{height:calc(66.66667% - 1.2em);width:66.66667%;transform:scale(1.5);transform-origin:0 0;border:0}.bottom-bar{background-color:#3A89C9;color:#fff;position:absolute;bottom:0;left:0;right:0;font-size:20px;padding:.8em;text-align:left;z-index:1}.bottom-bar p{margin:0}.full .bottom-bar{display:none}.side-layer{position:absolute;left:0;width:100%;padding:0 2em}.middle,.middle img,.middle span{vertical-align:middle}.top{vertical-align:top}.bottom{vertical-align:bottom}.inline-block p,.inline-block ul,.inline-block ol,.inline-block blockquote{display:inline-block;text-align:left}.no-margin,.no-margin>p,.no-margin>pre,.no-margin>ul,.no-margin>ol{margin:0}.no-padding{padding:0}.space-left{padding-left:1em}.space-right{padding-right:1em}.responsive>img{width:100%;height:auto}.contain{background-size:contain}.overlay{box-shadow:inset 0 0 0 9999px rgba(0,0,0,0.5)}.left{text-align:left}.right{text-align:right}.center{text-align:center}.justify{text-align:justify}.primary{color:#3A89C9}.alt{color:#F26C4F}.em{color:#757575}.thin{font-weight:200}.huge{font-size:2em}.big{font-size:1.5em}.small{font-size:.8em}.dark-bg{background-color:#333}.alt-bg{background-color:#F26C4F}.row{width:100%}.row::after{content:'';display:table;clear:both}.row.table{display:table}.row.table [class^="col-"]{float:none;display:table-cell;vertical-align:inherit}[class^="col-"]{float:left}[class^="col-"].inline-block{float:none;display:inline-block}.col-1{width:8.33333%}.col-2{width:16.66667%}.col-3{width:25%}.col-4{width:33.33333%}.col-5{width:41.66667%}.col-6{width:50%}.col-7{width:58.33333%}.col-8{width:66.66667%}.col-9{width:75%}.col-10{width:83.33333%}.col-11{width:91.66667%}.col-12{width:100%}@keyframes fadeIn{from{opacity:0}to{opacity:1}}.animation-fade{animation-duration:300ms;animation-fill-mode:both;animation-timing-function:ease-out}.remark-visible .animation-fade{animation-name:fadeIn}@page{size:1210px 681px;margin:0}@media print{.remark-slide-scaler{width:100% !important;height:100% !important;transform:scale(1) !important;top:0 !important;left:0 !important}}.bottom-bar a{color:white;text-decoration:none}.img-center{display:block;margin:0 auto;max-width:90%}.img-squash{height:460px}.img-small{width:40%}.slack blockquote{font-size:40px}.slack-efficiency h1{font-size:80px}.white-title h1{color:white}.white-title-outline h1,.white-title-outline h2{color:white;text-shadow:-4px -4px 0 #000, 4px -4px 0 #000, -4px 4px 0 #000, 4px 4px 0 #000}.white-link a{color:white}.writing a{font-size:1.3em}.spring-cleaning{background-position:bottom}.outline ul{padding-left:130px}.distributed{display:flex;justify-content:space-between}.lowered{padding-top:90px}

</style>
  <script src="file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/template/remark.min.js"></script>
  <script>
    function create() {
      var slideshow = remark.create({
        source: "title: A Shallow Dive Into Deep Learning\nurl: https://www.lib.ncsu.edu\nclass: animation-fade\nlayout: true\n\n<!-- This slide will serve as the base layout for all your slides -->\n.bottom-bar[\n  {{title}}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{url}}\n]\n\n---\n# A Shallow Dive Into Deep Learning\n\n## Kevin Beswick (@kbeswick)\n## Bret Davidson\n\n### NCSU Libraries\n\nTo see the speaker notes press \"p\"\n\n???\nBD Start\n\nGreat you're seeing the speaker notes!\n\nHere are some notes for the speakers to control the slide deck:\n\n- c: Create a clone presentation on a new window\n- p: Toggle PresenterMode\n- f: Toggle Fullscreen\n- t: Reset presentation timer\n\n---\n\n<img class=\"img-center img-squash\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/hype2.png\"></img>\n\n???\n\nYou've probably heard a lot about machine learning in the news, and have\nundoubtedly interacted with things that use it. We're going to talk\nabout what it is, what it can be used for, how it works and its\napplication in libraries.\n\n---\n# Computers are good at:\n\n- Calculations\n- Data storage and retrieval\n- Displaying data\n- Automation of routine tasks\n\n# Computers are bad at:\n\n- Pattern recognition\n- Perception\n- Generalization\n- Inference\n\n???\n\nLet's set the stage a little bit. Computers have traditionally been good at calculations, data storage, retrieval and display, automating things that used to take a while to do manually, and following well defined rules and structures.\n\nComputers have traditionally been worse at pattern recognition, perception (knowing what something is), generalization, and inference (making predictions).\n\nThese are some of the areas where deep learning is helping to improve the performance of computers.\n\n---\nclass:impact\n# SumaVision Demo\n\n???\nhttps://dli-ml.lib.ncsu.edu/sumavision/\n\nWe'll start off with a demo that maybe 5-7 years ago would have been very hard to accomplish using traditional computing tools.\n\nTo give some context, Suma is an application we developed at NC State for doing space analytics. Generally the way this is done is that someone walks through the library with an IPad and observes with their own eyes what people are doing in our spaces. They manually capture the number of people by navigating the space and pressing a button for each person they see.\n\n---\n# SumaVision Demo\n\n<video controls src=\"videos/sumav_1.mov\" type=\"video/mp4\" class=\"img-squash img-center\"/>\n\n???\n\nLeaving ethical considerations aside for the purposes of a technical demo, what if we could capture images of a space and use that to count the number of people in a space?\n\nWe built this demo to show that we can do something like this pretty easily. You can see that this was captured in Suma.\n\n---\n# Machine Learning\n\n- “A field of computer science that gives computers the ability to learn without being explicitly programmed”\n  - https://en.wikipedia.org/wiki/Machine_learning\n\n## Supervised Learning\n\n- Making a prediction based on labeled training data\n\n## Unsupervised Learning\n\n- Describing hidden structure from \"unlabeled\" data\n\n???\n\nSupervised learning is when we have an algorithm that learns how to make predictions based on labeled data it has access to in advance. Examples of these are things like linear regression, logistic regression, and random forests.\n\nThe other broad category is unsupervised learning, which is trying to describe hidden structures in unabled data and then make a prediction. An example of this might be classifying unlabeled textual data.\n\nThe majority of practical ML applications are supervised learning. If we have an algorithm and we feed it labeled images of cats and dogs, we can train it to evaluate a previously unseen, new image and classify it.\n\n---\n# Deep Learning\n<img class=\"img-center\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/fully_connected_network_diagram.png\"></img>\n\n???\nSubfield of machine learning that involves the use of deep artificial neural networks.\n\nLoosely mimics how the human brain works with layers of neurons with connections between them.\n\nDeep learning algorithms are neural networks, and they are a type of supervised learning. We train them with labeled data and then we make predictions on unlabeled data.\n\n---\n# Deep Learning vs. Traditional Machine Learning\n\n* Generalizable\n* More Powerful\n* Domain Neutral\n* Task Agnostic\n\n???\n\nDeep learning is generalizable and more powerful than traditional machine learning.\n\nIn traditional machine learning we have to manually define features, which is time intensive and requires domain expertise. Deep learning algorithms learn features automatically.\n\nNeural networks don't need to know anything about the problem domain they are working in. In fact, they don't even know that they are operating on images. All they see are numbers.\n\nThe same deep learning algorithms can be used for different tasks. If I wanted to have an algorithm to tell me if an image is a cat or a dog, it could also tell me if something was a hot dog or a pizza.\n\nThe code doesn't need to change, only the data being used to train the network.\n\n---\nclass:impact\n# What is deep learning good for anyway?\n\n???\nLet's talk about what problems deep learning is good at solving and specific examples of its applications.\n\n---\n\n# Computer Vision\n- Image classification\n- Object detection/localization\n- Image captioning\n\n<img class=\"img-small\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/uber.png\"></img>\n<img class=\"img-small\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/medical-image.jpg\"></img>\n\n???\n\nOne area is computer vision, which is concerned with recognizing what is in an image and where objects are within an image.\n\nThis is used in areas like image classification, object detection, self-driving cars, and medical imagery.\n\n---\n# Natural Language Processing\n  - Machine translation\n  - Language modeling\n  - Word embedding\n\n\n <img class=\"img-small\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/translate.png\"></img>\n\n???\nDeep learning is also used for NLP problems like translation and identifying concept similarity within text.\n\n---\n# Audio processing\n  - Speech-to-text\n  - Text-to-speech\n\n<img class=\"img-small\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/siri.jpg\"></img>\n\n???\nDigital assistant examples fall here. For example, take an audio file of a human voice and put it into text so a machine can act on it.\n\n---\nclass: impact\n# How do neural networks work?\n\n???\nKB Start\n\nLet's take a look at how deep neural networks actually work.\n\n---\n\n# High Level Process\n\n- Define a prediction problem: given x, can I predict y?\n  - Example: given an image, can I predict whether it is of a cat or a dog?\n\n- Gather training data\n  - Images of cats and dogs that are already labeled \"cat\" or \"dog\"\n\n- Given this set of labeled training data, train a model that can\n  make predictions given new, unseen images.\n\n???\n\nThe general process we'll have to go through to train a neural network\nis as follows:\n\nread slide\n\n---\n\n# Everything is Numbers\n\n<img class=\"img-center img-squash\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/numeric_representation.png\"></img>\n\n???\n\nThe first point I want to make is that everything is numbers to a neural\nnetwork. So we're making predictions on things like images and text, but\nfirst we need to represent these numerically. Computers already do this\nbehind the scenes in a lot of cases.\n\nFor black and white images, we can represent them as a matrix of numbers, where each\nnumber represents the intensity of a particular pixel (or how light/dark\nit is). For colored images, each number would be a set of 3 numbers that\nrepresent the intensity of Red, Green, and Blue colors in a given pixel.\n\nThere are similar approaches for text and other data.\n\n---\n\n# Neural Network Model\n\n<img class=\"img-center img-squash\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/nn_feed_forward.png\"></img>\n\n???\n\nHere's what a standard neural network looks like. They are organized as\nstacked layers of neurons, with connections between them. Each of these\nconnections has a numerical weight that represents how strong the\nconnection is, and these are what we need to learn as part of the\ntraining process.\n\nOur input image is fed into the network through the input layer on the left. The data flows\nthrough the neurons and connections in the hidden layers in the middle, until a prediction is made at the output layer at the\nend of the network.\n\nIn each neuron, the weights are applied to incoming values, they are\ncombined and sent through an \"activation function\" which either mutes or\namplifies the signal going into the next neuron.\n\nEssentially, we want to change the weights such that when we feed in an\nimage of a dog, the numbers flow through the network in such a way that\nthe number for \"dog\" in the output layer is closer to 1. We can do this\nusing a process called back propagation over the entire training set\nmany times.\n\n---\n\n# Trained Model\n\n- Weights are set to values such that model makes good predictions on\n  training set\n  - Training set should be a representative view of reality in order to\n    generate a good model\n- Inference\n  - Can now run unseen examples through model to get predictions\n- Single purpose\n  - Can recognize cats and dogs, but not horses\n  - But can just add images of horses to training set, add third output\n    class, and retrain model\n\n???\n- Now that the model has been trained on a large representative dataset,\n  it is very good at distinguishing between cats and dogs.\n- Our model is now ready to be put into production within an application that will feed it new data.\n  The process of getting predictions from your model on unseen data is called inference.\n- But if you showed it a picture of a horse, it would be very confused, since it\n  has never seen a horse before. It would likely report low confidence\n  scores for both cat and dog\n- If you needed the ability to recognize horses, you would add a third\n  node to the output layer, expand your training dataset to include\n  labeled pictures of horses, and retrain the model.\n- There is no need to write a manual horse recognition algorithm and\n  integrate it to your application, you can just retrain the\n  network.\n\n---\nclass:impact\n# How do neural networks learn?\n\n???\nBD Start\n\nNow that we have an understanding of how neural networks work, we've prepared a demo that will help us understand how neural networks learn and how a model performs at different stages of development.\n\n---\nbackground-image: url(file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/mariokart.jpg)\n\n???\nSelf-driving Mario Kart! Why did we think this would be a good example?\n\n- needed a way to create a large amount of labeled input data quickly\n- visualize the performance of the model using different sizes of data sets\n- seemed better than putting Kevin's son Elliott on a self driving tractor\n\n---\n# How do we do this?\n\n- Model is trained using inputs:\n  - Screenshots of the game taken at prescribed intervals (.2 seconds)\n  - Controller input (joystick angle and which buttons are pressed)\n\n- Predictions are made:\n  - Given NEW, untrained screenshot, generate NEW joystick inputs\n\n???\nWe wrote a program that took a screenshot of the game every 0.2 seconds, while at the same time recording the controller input.\n\nThe model then makes predictions given NEW, untrained screenshots, and generates NEW joystick inputs.\n\n---\n<video controls src=\"videos/1.mov\" type=\"video/mp4\" class=\"img-squash img-center\"/>\n\n???\nThis is an early stage demo trained on a single lap around the track. It hasn't learned much yet, it's not turning.\n\n---\n<video controls src=\"videos/2.mov\" type=\"video/mp4\" class=\"img-squash img-center\"/>\n\n???\nThis example is a model that has been trained on 3 laps around the track. You can see it can now do basic turns.\n\nA few notes on how this is working. You are seeing the prediction part of deep learning. We are taking rapid screenshots of the game, passing the image (as a multi-dimensional array of numbers) to our model, getting a prediction (controller input), and then sending that input into MarioKart.\n\nThe window on the right is showing the input that is being sent. Notice how quickly the predictions are being made, every line in the terminal output is a prediction.\n\n---\n<video controls src=\"videos/3.mov\" type=\"video/mp4\" class=\"img-squash img-center\"/>\n\n???\n~15 mins of play\n\nThis example performs much better, and can do things like error recovery. This one can actually finish a race.\n\n---\nclass: impact\n# Opportunities in Libraries\n\n???\nKB Start\n\nA major part of our initial exploration in this area was to identify some of the opportunities in libraries.\n\nWe've found three major categories.\n\n---\nclass:impact\n# New functionality\n\n???\n\nFirst is integrating deep learning into our own applications to get some new functionality we couldn't get before. We've mostly been looking at automatic generation of metadata or analyzing media like images, audio and video so far.\n\n---\n\n# WolfTales\n\n<video controls src=\"http://siskel.lib.ncsu.edu/SCRC/mc00581-wt-peele-20151030/mc00581-wt-peele-20151030.mp4\" type=\"video/mp4\" class=\"img-sqash img-center\" />\n\n???\n\nFirst, lets look at a few seconds of this Wolf Tales video. I want you\nto pay attention to some of the key things he mentions.\n\n---\n\n# Audio/Video Processing\n\n<video controls src=\"videos/deep_catalog_1_720.mov\" type=\"video/mp4\" class=\"img-squash img-center\"/>\n\n???\nHere's a demo catalog application we've developed to show how we could leverage\ndeep learning to get a head start in metadata generation for newly\ndigitized or created materials, and how we could improve discovery\nwithout any human generated metadata.\n\nI'm going to ingest this video and only give it a title and author.\n\nSo what is happening here? When I uploaded the video, in the background, the audio was extracted automatically and it was run through a speech to text model. The full text is being indexed into Solr and now I can search for things that we heard in the video.\n\nNow that we have a textual transcription, imagine what else we can do.\nWe can definitely provide it directly to users and automatically enable\ncaptioning on the video. We can do further analysis on that text, and\ngenerate recommendations for appropriate subject headings, or at least\nget the key terms or create a summary in an automated way.\n\n---\n# Newspapers\n\n<video controls src=\"videos/deep_catalog_3_720.mov\" type=\"video/mp4\" class=\"img-squash img-center\"/>\n\n???\n\nHere's another.\nThis one uses the same model architecture as\nSumaVision, but we took off the later layers and retrained on new data\nwe collected.\n\nThis one finds the location of headlines and images in\nnewspapers. We can then run further processing to find out what is in\nthe image, and to OCR the headlines. We can then offer more fine grained\nsearch results based on the articles in a newspaper, and the ability to\njump to that specific article automatically since we know what page its\non in the newspaper and where on the page it is.\n\n---\n\nclass:impact\n# Supporting Researchers\n\n\n???\nThe second opportunity for libraries is supporting researchers through deep learning consultations and research sprints.\n\n---\n# Snowflake Classification\n\n<span class=\"distributed\">\n    <img class=\"lowered\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/snow2.png\"></img>\n    <img class=\"lowered\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/snow3.png\"></img>\n    <img class=\"lowered\" src=\"file:///Users/bddavids/code/shallow-dive-deep-learning-dlf2018/images/snow1.png\"></img>\n</span>\n\n???\n\nA faculty member at NCSU from the dept of marine, earth, and atmospheric sciences, contacted the libraries looking for machine learning support. They have an extremely large dataset of over 1 million snowflake images.\n\nThey have used a number of \"traditional\" machine learning techniques to attempt to classify degrees of \"riming\" on snowflakes, that is, how large or small the clusters of ice are. We are working with them to develop a proof of concept deep learning model to further improve on their results.\n\nThis has also been an opportunity to explore the viability of providing this kind of service to researchers. Is it useful for them? Can we scale this kind of support?\n\n---\nclass:impact\n# Cultural Heritage Ecosystem\n\n???\nA third opportunity area is developing the ecosystem around deep learning use in cultural heritage institutions\n\n---\n# Data Annotation and Sharing\n- opportunities for defining best practices for sharing models and approaches\n- using standards like IIIF and WebAnnotation/OpenAnnotation\n\n???\n\nTo encourage sharing of models and data, we can develop best practices that include standards that are already being used in libraries, such as IIIF. We've been developing approaches that use collections of IIIF image URLs as training datasets rather than static copies of images. This allows for a distributed training data set potentially spanning over many institutions, and simplifies the data sharing process by reducing its size.\n\nWe are also having our deep learning services output annotations in standardized formats like WebAnnotation or OpenAnnotation so that they can be viewed in existing image viewers and be provided in a consistent format.\n\n---\n# Ethics\n\n- data is often the source of bias in this technology\n- identify ways to create more representative data sets\n- expose to the user that we are using this technology\n- give them the option to provide feedback\n- give them the option to turn it off\n\n???\nWe wanted to close by saying a bit about algorithmic bias in deep learning. We hope we've convinced you through this presentation that the data used to train models is where bias is often introduced.\n\nWhat are some concrete steps we can take to reduce the potential bias of these systems and their impact on users?\n\nWe can create more representative data sets.\n\nWe can make it clear to the user when we are using deep learning in our services.\n\nWe can give users the option to provide feedback, which could influence how the model is trained in the future.\n\nWe can give users the option to turn off deep learning enhancements.\n\n---\n\nclass:impact\n\n# Thanks!\n\n## Kevin Beswick (kdbeswic@ncsu.edu)\n## Bret Davidson (bddavids@ncsu.edu)\n",
        ratio: '16:9',
        highlightLines: true,
        countIncrementalSlides: false,
        highlightStyle: 'github'
      });

      var slideElements;

      function getElementForSlide(slide) {
        slideElements = slideElements || document.querySelectorAll('.remark-slide')
        return slideElements[slide.getSlideIndex()]
      }

      slideshow.on('showSlide', function (slide) {
        Array.from(getElementForSlide(slide).querySelectorAll('video, audio')).forEach(function (vid) {
          vid.loop = true
          vid.currentTime = 0
          vid.play()
        })
      });

      slideshow.on('hideSlide', function (slide) {
        Array.from(getElementForSlide(slide).querySelectorAll('video, audio')).forEach(function (vid) {
          vid.pause()
        })
      });

      return slideshow;
    }
  </script>
</head>
<body onload="slideshow = create()">
</body>
</html>
